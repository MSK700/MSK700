
#Importing necessary libraries to be used (if you dont have these libraries installed first install them by using; install.packages("libraryname"))

library(tidyverse)
library(corrplot)
library(data.table)
library(readr)
library(dplyr)
library(caret)
library(glmnet)
library(party)
library(ipred)
library(randomForest)

#Part 1 – General data preparation and cleaning.
##a) Import the MLDATASET_PartiallyCleaned.xlsx into R Studio. This dataset is a partially cleaned version of MLDATASET-200000-1612938401.xlsx.
#Viewing the data
##We name the initial partially cleaned data set as MLini
MLini <- read.csv("C:/Users/fe/Downloads/MLDATASET_PartiallyCleaned.csv", header = T)
View(MLini)
str(MLini) 
summary(MLini)
summary(MLini$How.Many.Times.File.Seen)
summary(MLini$Threads.Started)

#using hist function to draw histogram to check the data
hist(MLini$How.Many.Times.File.Seen)
hist(MLini$Threads.Started)

MLini %>% select_if(is.numeric) %>% cor() %>% corrplot()

#b) Write the appropriate code in R Studio to prepare and clean the
#{MLDATASET_PartiallyCleaned dataset as follows:
#i. For How.Many.Times.File.Seen, set all values = 65535 to NA;
#ii. Convert Threads.Started to a factor whose categories are given by
#1 = 1 thread started
#2 = 2 threads started
#3 = 3 threads started
#4 = 4 threads started
#5 = 5 or more threads started
#Hint: Replace all values greater than 5 with 5, then use the factor(.) function.

#iii. Log-transform Characters.in.URL using the log(.) function, and remove the original Characters.in.URL column from the dataset (unless you have overwritten it with the log-transformed data)
#iv. Select only the complete cases using the na.omit(.) function, and name the dataset MLDATASET.cleaned.}

MLini$How.Many.Times.File.Seen[MLini$How.Many.Times.File.Seen>=65535] <- NA
#we figured that the value we replaced with NA was an outlier and have decided to remove it, an alternate method could have been to replace it by the median value

# using summary and histogram function to check if we have successfully removed the outlier with NA

summary(MLini$How.Many.Times.File.Seen)
hist(MLini$How.Many.Times.File.Seen)

# now doing the same i.e. removing the outlier from the threads started column and then converting it to a categorical variable. we say that any value above 5 is equal to 5 threads started

MLini$Threads.Started[MLini$Threads.Started>=5] <-5

hist(MLini$Threads.Started)

#converting the numeric variable to categorical variable

MLini$Threads.Started = as.factor(MLini$Threads.Started)

#normalizing the characters in URL by using the log function
hist(MLini$Characters.in.URL)
summary(MLini$Characters.in.URL)
MLini$Characters.in.URL = log(MLini$Characters.in.URL)

#removing all the NA and blank values by na.omit() function

ML.cleaned = na.omit(MLini)
str(ML.cleaned)

#{c) Write the appropriate code in R Studio to partition the data into training and test sets using an 30/70 split. Be sure to set the randomisation seed using your student ID. Export both the training and test datasets as csv files, and these will need to be submitted along with your code. Note that the training set is typically larger than the test set in practice. However, given the size of this dataset, you will only use 30% of the data to train your ML models to save time. }


ML.cleaned %>% select_if(is.numeric) %>% cor() %>% corrplot()
ggplot(ML.cleaned, aes(x = Download.Speed, y = File.Size.Bytes, fill = Threads.Started)) + geom_boxplot()
ggplot(ML.cleaned, aes(x =Actually.Malicious , y = File.Size.Bytes, fill = Initial.Statistical.Analysis)) + geom_boxplot()
ggplot(ML.cleaned, aes(x = `Actually.Malicious`, y = `File.Size.Bytes`, fill=`Initial.Statistical.Analysis`)) + 
  geom_bar(stat="identity") + theme_minimal()

#now that we have cleaned the data we will create test and train data before going on training the models; the instructions are against practice because it says to take 30% in train, usually we take 70% in train and 30% in test

set.seed(10288996)

train_ind = rbinom(nrow(ML.cleaned), 1, 0.3)

train.data = subset(ML.cleaned, train_ind == 1)[,-c(1,16)]

test.data = subset(ML.cleaned, train_ind == 0)[,-c(1,16)]

str(train.data)

str(test.data)

nrow(train.data)

nrow(test.data)

# now that we have the partitions ready we use it for the the first model list i.e. lasso,ridge and elastic net, the difference between them is that by the value of alpha, we also remove the first and last row as per the intructions and to remove unneccesary complexity

x <- model.matrix(Actually.Malicious~., train.data)

y <- ifelse(train.data$Actually.Malicious == "YES", 1, 0)

x.test <- model.matrix(Actually.Malicious ~., test.data)

#checking for personal inquiry

summary(test.data$Actually.Malicious)


#{Part 2 – Compare the performances of different machine learning algorithms
#{a) Select three supervised learning modelling algorithms to test against one another by running the following code. Make sure you enter your student ID into the command set.seed(.). Your 3 modelling approaches are given by myModels.}
#Str of given structure of model
set.seed(10288996)
models.list1 <- c("Logistic Ridge Regression",
                  "Logistic LASSO Regression",
                  "Logistic Elastic-Net Regression")
models.list2 <- c("Classification Tree",
                  "Bagging Tree",
                  "Random Forest")
myModels <- c("Binary Logistic Regression",
              sample(models.list1,size=1),
              sample(models.list2,size=1))
myModels %>% data.frame 
                                .
#1      Binary Logistic Regression
#2 Logistic Elastic-Net Regression
#3             Classification Tree}

#a) Run the ML algorithm in R on the training set with Actually.Malicious as the
#outcome variable. EXCLUDE Sample.ID and Initial.Statistical.Analysis from the
#modelling process. (Already removed in creating training and test data set)

#WITHOUT ANY TUNING:

#elastic net regression

cv.elastic.net.regression <- cv.glmnet(x, y, alpha = 0.3, family = "binomial")
probabilities.elastic.net.regression <- cv.elastic.net.regression %>% predict(newx = x.test, type= "response")
predicted.classes.elastic.net.regression <- as.factor(ifelse(probabilities.elastic.net.regression > 0.5, "YES", "NO"))
summary(cv.elastic.net.regression)
str(cv.elastic.net.regression)
coef(cv.elastic.net.regression)
View(predicted.classes.elastic.net.regression)
confusionMatrix(predicted.classes.elastic.net.regression, test.data$Actually.Malicious)

#ctree
Classification.Tree = ctree(Actually.Malicious ~ ., data = train.data)
pred.ctree = Classification.Tree %>% predict(test.data)
confusionMatrix(pred.ctree, test.data$Actually.Malicious)

#binarylogisticregression
str(train.data)
Binarylogisticregression = glm(Actually.Malicious ~ ., data = train.data, family = 'binomial', maxit = 25)

pred.Binarylogisticregression = Binarylogisticregression %>% predict(test.data, type = "response")

summary(test.data$Download.Speed)

summary(train.data$Download.Speed)

predictedclasses.Binarylogisticregression = as.factor(ifelse(pred.Binarylogisticregression > 0.5, "YES", "NO"))

confusionMatrix(predictedclasses.Binarylogisticregression, test.data$Actually.Malicious)

coef(Binarylogisticregression)

#HYPERPARAMETER TUNING!!!!!!!!!!!


###glmnet() is a R package which can be used to fit Regression models,lasso model and others. Alpha argument determines what type of model is fit. When alpha=0, Ridge Model is fit and if alpha=1, a lasso model is fit.

###cv.glmnet() performs cross-validation, by default 10-fold which can be adjusted using nfolds. A 10-fold CV will randomly divide your observations into 10 non-overlapping groups/folds of approx equal size. The first fold will be used for validation set and the model is fit on 9 folds. Bias Variance advantages is usually the motivation behind using such model validation methods. In the case of lasso and ridge models, CV helps choose the value of the tuning parameter lambda.

#elastic net regression

cv.elastic.net.regression1 <- cv.glmnet(x, y, alpha = 0.3, family = "binomial", lambda = cv.elastic.net.regression$lambda)
probabilities.elastic.net.regression1 <- cv.elastic.net.regression1 %>% predict(newx = x.test, type= "response")
predicted.classes.elastic.net.regression1 <- as.factor(ifelse(probabilities.elastic.net.regression1 > 0.5, "YES", "NO"))
summary(cv.elastic.net.regression1)
str(cv.elastic.net.regression1)
coef(cv.elastic.net.regression1)
View(predicted.classes.elastic.net.regression1)
confusionMatrix(predicted.classes.elastic.net.regression1, test.data$Actually.Malicious)

#ctree1
Classification.Tree1 = ctree(Actually.Malicious ~ ., data = train.data[,-5], control = ctree_control(mincriterion = 0.90))
pred.ctree1 = Classification.Tree1 %>% predict(test.data)
confusionMatrix(pred.ctree1, test.data$Actually.Malicious)
plot(Classification.Tree)

#{c) Evaluate the performance of each ML models on the test set. Provide the confusion matrices and report the following:
# Sensitivity (the detection rate for actual malicious samples)
# Specificity (the detection rate for actual non-malicious samples)
# Overall Accuracy}
confusionMatrix(predicted.classes.elastic.net.regression, test.data$Actually.Malicious)
confusionMatrix(pred.ctree, test.data$Actually.Malicious)
confusionMatrix(predictedclasses.Binarylogisticregression, test.data$Actually.Malicious)
confusionMatrix(predicted.classes.elastic.net.regression1, test.data$Actually.Malicious)
confusionMatrix(pred.ctree1, test.data$Actually.Malicious)

#d) Provide a brief statement on your final recommended model and why you chose that model over the others. Parsimony, accuracy, and to a lesser extent, interpretability should be taken into account.
##I will choose the tuned classification Tree model because its showing the highest Accuracy, Sensitivity and specificity, but this could become even better if we know the actual cost related to the prediction i.e. what will cost if we are unable to identify a malware and what in terms of cost will we benefit from correctly identifying the malware so we can tune better by choosing our threshold and using the optimizing function on the elastic net and logistic regression to change their coefficient values.


#e) Create a confusion matrix for the variable Initial.Statistical.Analysis in the test set.Recall that the data in this column correspond to TOBORRM’s initial attempt to classify the samples. Compare and comment on the performance of your optimal ML model in part d) to the initial analysis by the TOBORRM team.

test2 = subset(ML.cleaned, train_ind == 0)[,c(15,16)]
str(test2)
summary(test2$Initial.Statistical.Analysis)
test2$ini[test2$Initial.Statistical.Analysis == "Correctly Identified as Clean"] = "NO"
test2$ini[test2$Initial.Statistical.Analysis == "Correctly Identified as Malware"] = "YES"
test2$ini[test2$Initial.Statistical.Analysis == "Incorrectly Identified as Clean"] = "NO"
test2$ini[test2$Initial.Statistical.Analysis == "Incorrectly Identified as Malware"] = "YES" 
test2$ini = as.factor(test2$ini)
confusionMatrix(test2$ini, test2$Actually.Malicious)